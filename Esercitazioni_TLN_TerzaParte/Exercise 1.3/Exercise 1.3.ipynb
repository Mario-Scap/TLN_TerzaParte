{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.spatial import distance\n",
    "import matplotlib.pyplot\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rimuovo la punteggiatura\n",
    "def remove_punctuation(sentence):\n",
    "    return re.sub(r'[^\\w\\s]', '', sentence)\n",
    "\n",
    "#rimuovo le stopwords\n",
    "def remove_stopwords(words_list):\n",
    "    stopwords = open(\"stop_words_FULL.txt\", \"r\") \n",
    "    stopwords_list = [] \n",
    "    for word in stopwords:\n",
    "        stopwords_list.append(word.replace('\\n', ''))\n",
    "    stopwords.close()\n",
    "    return [value.lower() for value in words_list if value.lower() not in stopwords_list] #restituisco la lista di stopwords trovate\n",
    "    \n",
    "#effettuo lo splitting della frase sfruttando gli spazi e poi ogni parola viene portare al suo lemma\n",
    "def tokenize_sentences(sentence):\n",
    "    word_list= []\n",
    "    lemma = WordNetLemmatizer()\n",
    "    for tag in nltk.pos_tag(word_tokenize(sentence)):\n",
    "        if(tag[1][:2] == 'NN'):\n",
    "            word_list.append(lemma.lemmatize(tag[0], pos = wn.NOUN))\n",
    "        elif(tag[1][:2] == 'VB'):\n",
    "            word_list.append(lemma.lemmatize(tag[0], pos = wn.VERB))\n",
    "        elif(tag[1][:2] == 'RB'):\n",
    "            word_list.append(lemma.lemmatize(tag[0], pos = wn.ADV))\n",
    "        elif(tag[1][:2] == 'JJ'):\n",
    "            word_list.append(lemma.lemmatize(tag[0], pos = wn.ADJ))\n",
    "    \n",
    "    return word_list\n",
    "\n",
    "#operazione di preprocessing che effettua le operazioni di Text Cleaning svolte precedentemente\n",
    "def pre_processing (sentence):\n",
    "    return remove_stopwords(tokenize_sentences(remove_punctuation(sentence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def definitons_lenght(word):\n",
    "    lens = []\n",
    "\n",
    "    for sys in wn.synsets(word):\n",
    "\n",
    "        path= []\n",
    "\n",
    "        #for i in range(0, len(sys.definition().split(\" \"))):\n",
    "        path.append((sys.definition(),len(sys.definition().split(\" \"))))\n",
    "\n",
    "        print(\"Term {}\".format(sys), \":\" ,path)\n",
    "        print()\n",
    "        lens.append(path)\n",
    "\n",
    "    return lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------\n",
      "\n",
      "Concept:  Courage\n",
      "Term Synset('courage.n.01') : [('a quality of spirit that enables you to face danger or pain without showing fear', 15)]\n",
      "\n",
      "\n",
      "------------------------\n",
      "\n",
      "Concept:  Paper\n",
      "Term Synset('paper.n.01') : [('a material made of cellulose pulp derived mainly from wood or rags or certain grasses', 15)]\n",
      "\n",
      "Term Synset('composition.n.08') : [('an essay (especially one written as an assignment)', 8)]\n",
      "\n",
      "Term Synset('newspaper.n.01') : [('a daily or weekly publication on folded sheets; contains news and articles and advertisements', 14)]\n",
      "\n",
      "Term Synset('paper.n.04') : [('a medium for written communication', 5)]\n",
      "\n",
      "Term Synset('paper.n.05') : [('a scholarly article describing the results of observations or stating hypotheses', 11)]\n",
      "\n",
      "Term Synset('newspaper.n.02') : [('a business firm that publishes newspapers', 6)]\n",
      "\n",
      "Term Synset('newspaper.n.03') : [('the physical object that is the product of a newspaper publisher', 11)]\n",
      "\n",
      "Term Synset('paper.v.01') : [('cover with paper', 3)]\n",
      "\n",
      "Term Synset('wallpaper.v.01') : [('cover with wallpaper', 3)]\n",
      "\n",
      "\n",
      "------------------------\n",
      "\n",
      "Concept:  Apprehension\n",
      "Term Synset('apprehension.n.01') : [('fearful expectation or anticipation', 4)]\n",
      "\n",
      "Term Synset('understanding.n.01') : [('the cognitive condition of someone who understands', 7)]\n",
      "\n",
      "Term Synset('apprehension.n.03') : [('painful expectation', 2)]\n",
      "\n",
      "Term Synset('apprehension.n.04') : [('the act of apprehending (especially apprehending a criminal)', 8)]\n",
      "\n",
      "\n",
      "------------------------\n",
      "\n",
      "Concept:  Sharpener\n",
      "Term Synset('sharpener.n.01') : [('any implement that is used to make something (an edge or a point) sharper', 14)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for w in ['Courage', 'Paper', 'Apprehension', 'Sharpener']: \n",
    "    print(\"\\n------------------------\\n\")\n",
    "    print(\"Concept: \", w)\n",
    "    definitons_lenght(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_hypernym_paths(word):\n",
    "    \n",
    "    lens = []\n",
    "    \n",
    "    for syn in wn.synsets(word):\n",
    "        path = []\n",
    "        \n",
    "        hyp_path = syn.hypernym_paths()\n",
    "        \n",
    "        for i in range (0, len(hyp_path[0])):\n",
    "            \n",
    "            path.append((hyp_path[0][i],len((hyp_path[0][i].definition()).split())))\n",
    "\n",
    "\n",
    "        print(\"hypernym distance from root: \\n\", (syn.hypernym_distances()))\n",
    "\n",
    "        print()\n",
    "        \n",
    "        print(path)\n",
    "        print()\n",
    "        lens.append(path)\n",
    "\n",
    "    return lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------\n",
      "\n",
      "Concept:  Courage\n",
      "hypernym distance from root: \n",
      " {(Synset('courage.n.01'), 0), (Synset('trait.n.01'), 3), (Synset('attribute.n.02'), 4), (Synset('abstraction.n.06'), 5), (Synset('entity.n.01'), 6), (Synset('spirit.n.03'), 1), (Synset('character.n.03'), 2)}\n",
      "\n",
      "[(Synset('entity.n.01'), 17), (Synset('abstraction.n.06'), 11), (Synset('attribute.n.02'), 9), (Synset('trait.n.01'), 7), (Synset('character.n.03'), 18), (Synset('spirit.n.03'), 9), (Synset('courage.n.01'), 15)]\n",
      "\n",
      "\n",
      "------------------------\n",
      "\n",
      "Concept:  Paper\n",
      "hypernym distance from root: \n",
      " {(Synset('paper.n.01'), 0), (Synset('matter.n.03'), 3), (Synset('part.n.01'), 3), (Synset('entity.n.01'), 6), (Synset('entity.n.01'), 5), (Synset('relation.n.01'), 4), (Synset('physical_entity.n.01'), 4), (Synset('substance.n.01'), 2), (Synset('material.n.01'), 1), (Synset('abstraction.n.06'), 5)}\n",
      "\n",
      "[(Synset('entity.n.01'), 17), (Synset('physical_entity.n.01'), 6), (Synset('matter.n.03'), 7), (Synset('substance.n.01'), 11), (Synset('material.n.01'), 12), (Synset('paper.n.01'), 15)]\n",
      "\n",
      "hypernym distance from root: \n",
      " {(Synset('written_communication.n.01'), 3), (Synset('writing.n.02'), 2), (Synset('communication.n.02'), 4), (Synset('essay.n.01'), 1), (Synset('abstraction.n.06'), 5), (Synset('entity.n.01'), 6), (Synset('composition.n.08'), 0)}\n",
      "\n",
      "[(Synset('entity.n.01'), 17), (Synset('abstraction.n.06'), 11), (Synset('communication.n.02'), 12), (Synset('written_communication.n.01'), 10), (Synset('writing.n.02'), 24), (Synset('essay.n.01'), 6), (Synset('composition.n.08'), 8)]\n",
      "\n",
      "hypernym distance from root: \n",
      " {(Synset('newspaper.n.01'), 0), (Synset('medium.n.01'), 3), (Synset('entity.n.01'), 9), (Synset('whole.n.02'), 6), (Synset('artifact.n.01'), 5), (Synset('physical_entity.n.01'), 8), (Synset('instrumentality.n.03'), 4), (Synset('print_media.n.01'), 2), (Synset('press.n.02'), 1), (Synset('object.n.01'), 7)}\n",
      "\n",
      "[(Synset('entity.n.01'), 17), (Synset('physical_entity.n.01'), 6), (Synset('object.n.01'), 12), (Synset('whole.n.02'), 11), (Synset('artifact.n.01'), 7), (Synset('instrumentality.n.03'), 13), (Synset('medium.n.01'), 9), (Synset('print_media.n.01'), 6), (Synset('press.n.02'), 16), (Synset('newspaper.n.01'), 14)]\n",
      "\n",
      "hypernym distance from root: \n",
      " {(Synset('paper.n.04'), 0), (Synset('artifact.n.01'), 3), (Synset('instrumentality.n.03'), 2), (Synset('physical_entity.n.01'), 6), (Synset('object.n.01'), 5), (Synset('medium.n.01'), 1), (Synset('whole.n.02'), 4), (Synset('entity.n.01'), 7)}\n",
      "\n",
      "[(Synset('entity.n.01'), 17), (Synset('physical_entity.n.01'), 6), (Synset('object.n.01'), 12), (Synset('whole.n.02'), 11), (Synset('artifact.n.01'), 7), (Synset('instrumentality.n.03'), 13), (Synset('medium.n.01'), 9), (Synset('paper.n.04'), 5)]\n",
      "\n",
      "hypernym distance from root: \n",
      " {(Synset('piece.n.06'), 2), (Synset('paper.n.05'), 0), (Synset('abstraction.n.06'), 7), (Synset('expressive_style.n.01'), 5), (Synset('object.n.01'), 6), (Synset('nonfiction.n.01'), 2), (Synset('article.n.01'), 1), (Synset('writing_style.n.01'), 4), (Synset('creation.n.02'), 3), (Synset('whole.n.02'), 5), (Synset('communication.n.02'), 6), (Synset('artifact.n.01'), 4), (Synset('prose.n.01'), 3), (Synset('entity.n.01'), 8), (Synset('physical_entity.n.01'), 7)}\n",
      "\n",
      "[(Synset('entity.n.01'), 17), (Synset('abstraction.n.06'), 11), (Synset('communication.n.02'), 12), (Synset('expressive_style.n.01'), 25), (Synset('writing_style.n.01'), 7), (Synset('prose.n.01'), 6), (Synset('nonfiction.n.01'), 6), (Synset('article.n.01'), 9), (Synset('paper.n.05'), 11)]\n",
      "\n",
      "hypernym distance from root: \n",
      " {(Synset('entity.n.01'), 9), (Synset('newspaper.n.02'), 0), (Synset('business.n.01'), 3), (Synset('abstraction.n.06'), 8), (Synset('enterprise.n.02'), 4), (Synset('organization.n.01'), 5), (Synset('publisher.n.01'), 1), (Synset('social_group.n.01'), 6), (Synset('group.n.01'), 7), (Synset('firm.n.01'), 2)}\n",
      "\n",
      "[(Synset('entity.n.01'), 17), (Synset('abstraction.n.06'), 11), (Synset('group.n.01'), 9), (Synset('social_group.n.01'), 5), (Synset('organization.n.01'), 7), (Synset('enterprise.n.02'), 6), (Synset('business.n.01'), 11), (Synset('firm.n.01'), 14), (Synset('publisher.n.01'), 6), (Synset('newspaper.n.02'), 6)]\n",
      "\n",
      "hypernym distance from root: \n",
      " {(Synset('artifact.n.01'), 3), (Synset('physical_entity.n.01'), 6), (Synset('newspaper.n.03'), 0), (Synset('product.n.02'), 1), (Synset('creation.n.02'), 2), (Synset('object.n.01'), 5), (Synset('whole.n.02'), 4), (Synset('entity.n.01'), 7)}\n",
      "\n",
      "[(Synset('entity.n.01'), 17), (Synset('physical_entity.n.01'), 6), (Synset('object.n.01'), 12), (Synset('whole.n.02'), 11), (Synset('artifact.n.01'), 7), (Synset('creation.n.02'), 10), (Synset('product.n.02'), 11), (Synset('newspaper.n.03'), 11)]\n",
      "\n",
      "hypernym distance from root: \n",
      " {(Synset('paper.v.01'), 0), (Synset('cover.v.01'), 1)}\n",
      "\n",
      "[(Synset('cover.v.01'), 9), (Synset('paper.v.01'), 3)]\n",
      "\n",
      "hypernym distance from root: \n",
      " {(Synset('cover.v.01'), 1), (Synset('wallpaper.v.01'), 0)}\n",
      "\n",
      "[(Synset('cover.v.01'), 9), (Synset('wallpaper.v.01'), 3)]\n",
      "\n",
      "\n",
      "------------------------\n",
      "\n",
      "Concept:  Apprehension\n",
      "hypernym distance from root: \n",
      " {(Synset('attribute.n.02'), 5), (Synset('state.n.02'), 4), (Synset('abstraction.n.06'), 6), (Synset('feeling.n.01'), 3), (Synset('fear.n.01'), 1), (Synset('emotion.n.01'), 2), (Synset('apprehension.n.01'), 0), (Synset('entity.n.01'), 7)}\n",
      "\n",
      "[(Synset('entity.n.01'), 17), (Synset('abstraction.n.06'), 11), (Synset('attribute.n.02'), 9), (Synset('state.n.02'), 10), (Synset('feeling.n.01'), 7), (Synset('emotion.n.01'), 3), (Synset('fear.n.01'), 20), (Synset('apprehension.n.01'), 4)]\n",
      "\n",
      "hypernym distance from root: \n",
      " {(Synset('psychological_feature.n.01'), 5), (Synset('process.n.02'), 3), (Synset('abstraction.n.06'), 6), (Synset('cognition.n.01'), 4), (Synset('higher_cognitive_process.n.01'), 2), (Synset('knowing.n.01'), 1), (Synset('entity.n.01'), 7), (Synset('understanding.n.01'), 0)}\n",
      "\n",
      "[(Synset('entity.n.01'), 17), (Synset('abstraction.n.06'), 11), (Synset('psychological_feature.n.01'), 10), (Synset('cognition.n.01'), 9), (Synset('process.n.02'), 14), (Synset('higher_cognitive_process.n.01'), 13), (Synset('knowing.n.01'), 6), (Synset('understanding.n.01'), 7)]\n",
      "\n",
      "hypernym distance from root: \n",
      " {(Synset('expectation.n.01'), 1), (Synset('psychological_feature.n.01'), 5), (Synset('entity.n.01'), 7), (Synset('content.n.05'), 3), (Synset('abstraction.n.06'), 6), (Synset('cognition.n.01'), 4), (Synset('belief.n.01'), 2), (Synset('apprehension.n.03'), 0)}\n",
      "\n",
      "[(Synset('entity.n.01'), 17), (Synset('abstraction.n.06'), 11), (Synset('psychological_feature.n.01'), 10), (Synset('cognition.n.01'), 9), (Synset('content.n.05'), 12), (Synset('belief.n.01'), 6), (Synset('expectation.n.01'), 8), (Synset('apprehension.n.03'), 2)]\n",
      "\n",
      "hypernym distance from root: \n",
      " {(Synset('act.n.02'), 3), (Synset('psychological_feature.n.01'), 5), (Synset('abstraction.n.06'), 6), (Synset('event.n.01'), 4), (Synset('capture.n.01'), 1), (Synset('acquiring.n.01'), 2), (Synset('apprehension.n.04'), 0), (Synset('entity.n.01'), 7)}\n",
      "\n",
      "[(Synset('entity.n.01'), 17), (Synset('abstraction.n.06'), 11), (Synset('psychological_feature.n.01'), 10), (Synset('event.n.01'), 9), (Synset('act.n.02'), 8), (Synset('acquiring.n.01'), 5), (Synset('capture.n.01'), 9), (Synset('apprehension.n.04'), 8)]\n",
      "\n",
      "\n",
      "------------------------\n",
      "\n",
      "Concept:  Sharpener\n",
      "hypernym distance from root: \n",
      " {(Synset('sharpener.n.01'), 0), (Synset('implement.n.01'), 1), (Synset('artifact.n.01'), 3), (Synset('instrumentality.n.03'), 2), (Synset('physical_entity.n.01'), 6), (Synset('object.n.01'), 5), (Synset('whole.n.02'), 4), (Synset('entity.n.01'), 7)}\n",
      "\n",
      "[(Synset('entity.n.01'), 17), (Synset('physical_entity.n.01'), 6), (Synset('object.n.01'), 12), (Synset('whole.n.02'), 11), (Synset('artifact.n.01'), 7), (Synset('instrumentality.n.03'), 13), (Synset('implement.n.01'), 12), (Synset('sharpener.n.01'), 14)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for w in ['Courage', 'Paper', 'Apprehension', 'Sharpener']: \n",
    "    print(\"\\n------------------------\\n\")\n",
    "    print(\"Concept: \", w)\n",
    "    total_hypernym_paths(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trovo gli antonimi relativi a ciascun concetto \n",
    "def find_antonyms(word):\n",
    "    antonyms = []\n",
    "    lens = []\n",
    "\n",
    "    for sys in wn.synsets(word):\n",
    "        for l in sys.lemmas():\n",
    "            if l.antonyms():\n",
    "                antonyms.append((l.antonyms()[0], len((l.antonyms()[0].name()).split())))\n",
    "\n",
    "    \n",
    "    print(\"Antonym : {}\".format(antonyms))\n",
    "    lens.append(antonyms)\n",
    "\n",
    "    return antonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------\n",
      "\n",
      "Concept:  Courage\n",
      "Antonym : [(Lemma('cowardice.n.01.cowardice'), 1)]\n",
      "\n",
      "------------------------\n",
      "\n",
      "Concept:  Paper\n",
      "Antonym : []\n",
      "\n",
      "------------------------\n",
      "\n",
      "Concept:  Apprehension\n",
      "Antonym : []\n",
      "\n",
      "------------------------\n",
      "\n",
      "Concept:  Sharpener\n",
      "Antonym : []\n"
     ]
    }
   ],
   "source": [
    "for w in ['Courage', 'Paper', 'Apprehension', 'Sharpener']: \n",
    "    print(\"\\n------------------------\\n\")\n",
    "    print(\"Concept: \", w)\n",
    "    find_antonyms(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vado a vedere la similiarita' tra le definizioni di iperonimi e le definizioni del concetto preso in analisi\n",
    "\n",
    "def similarity_search(word):\n",
    "\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2') \n",
    "\n",
    "    for sys in wn.synsets(word):\n",
    "        sim = 0\n",
    "\n",
    "        definitions = sys.definition()\n",
    "\n",
    "        print(\"concetto {} :\".format(sys))\n",
    "        #print('\\n')\n",
    "\n",
    "        hyper_list =sys.hypernyms()\n",
    "\n",
    "        for h in hyper_list:\n",
    "            hy_def = h.definition()\n",
    "            hyper_def_list = []\n",
    "\n",
    "            hyper_def_list.append(definitions)\n",
    "            hyper_def_list.append(hy_def)\n",
    "            hyper_def_list_sim = model.encode(hyper_def_list)\n",
    "\n",
    "            sim += 1 - distance.cosine(hyper_def_list_sim[0], hyper_def_list_sim[1])\n",
    "\n",
    "            if(len(hyper_list) != 0):\n",
    "                print(\"Average Similarity for hypernyms :\", sim/len(hyper_list))\n",
    "\n",
    "                print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------\n",
      "\n",
      "Concept:  Courage\n",
      "concetto Synset('courage.n.01') :\n",
      "Average Similarity for hypernyms : 0.4360363483428955\n",
      "\n",
      "\n",
      "------------------------\n",
      "\n",
      "Concept:  Paper\n",
      "concetto Synset('paper.n.01') :\n",
      "Average Similarity for hypernyms : 0.22500497102737427\n",
      "\n",
      "concetto Synset('composition.n.08') :\n",
      "Average Similarity for hypernyms : 0.4687023162841797\n",
      "\n",
      "concetto Synset('newspaper.n.01') :\n",
      "Average Similarity for hypernyms : 0.5600683093070984\n",
      "\n",
      "concetto Synset('paper.n.04') :\n",
      "Average Similarity for hypernyms : 0.49245941638946533\n",
      "\n",
      "concetto Synset('paper.n.05') :\n",
      "Average Similarity for hypernyms : 0.23190630972385406\n",
      "\n",
      "concetto Synset('newspaper.n.02') :\n",
      "Average Similarity for hypernyms : 0.7873624563217163\n",
      "\n",
      "concetto Synset('newspaper.n.03') :\n",
      "Average Similarity for hypernyms : 0.31530842185020447\n",
      "\n",
      "concetto Synset('paper.v.01') :\n",
      "Average Similarity for hypernyms : 0.5753957033157349\n",
      "\n",
      "concetto Synset('wallpaper.v.01') :\n",
      "Average Similarity for hypernyms : 0.40710264444351196\n",
      "\n",
      "\n",
      "------------------------\n",
      "\n",
      "Concept:  Apprehension\n",
      "concetto Synset('apprehension.n.01') :\n",
      "Average Similarity for hypernyms : 0.6164745092391968\n",
      "\n",
      "concetto Synset('understanding.n.01') :\n",
      "Average Similarity for hypernyms : 0.5371770262718201\n",
      "\n",
      "concetto Synset('apprehension.n.03') :\n",
      "Average Similarity for hypernyms : 0.2174414098262787\n",
      "\n",
      "concetto Synset('apprehension.n.04') :\n",
      "Average Similarity for hypernyms : 0.420794814825058\n",
      "\n",
      "\n",
      "------------------------\n",
      "\n",
      "Concept:  Sharpener\n",
      "concetto Synset('sharpener.n.01') :\n",
      "Average Similarity for hypernyms : 0.3430486023426056\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for w in ['Courage', 'Paper', 'Apprehension', 'Sharpener']: \n",
    "    print(\"\\n------------------------\\n\")\n",
    "    print(\"Concept: \", w)\n",
    "    similarity_search(w)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8713ca3b77628a4414cb19afecce4be886be5a2f883480e0c3ed66d57c8d45ba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
