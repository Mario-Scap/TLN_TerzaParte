{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "from statistics import mean\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from scipy.spatial import distance\n",
    "import matplotlib.pyplot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(sentence):\n",
    "    return re.sub(r'[^\\w\\s]', '', sentence)\n",
    "\n",
    "def remove_stopwords(words_list):\n",
    "    stopwords = open(\"stop_words_FULL.txt\", \"r\")\n",
    "    stopwords_list = []\n",
    "    for word in stopwords:\n",
    "        stopwords_list.append(word.replace('\\n', ''))\n",
    "    stopwords.close()\n",
    "    return [value.lower() for value in words_list if value.lower() not in stopwords_list]\n",
    "    \n",
    "#uso il tokenize_sentence perche poi mi servira' in futuro per la gestione delle frasi, con la relativa ricerca della parola piu frequente in una frase\n",
    "def tokenize_sentences(sentence):\n",
    "    word_list= []\n",
    "    lemma = WordNetLemmatizer()\n",
    "    for tag in nltk.pos_tag(word_tokenize(sentence)):\n",
    "        if(tag[1][:2] == 'NN'):\n",
    "            word_list.append(lemma.lemmatize(tag[0], pos = wn.NOUN))\n",
    "        elif(tag[1][:2] == 'VB'):\n",
    "            word_list.append(lemma.lemmatize(tag[0], pos = wn.VERB))\n",
    "        elif(tag[1][:2] == 'RB'):\n",
    "            word_list.append(lemma.lemmatize(tag[0], pos = wn.ADV))\n",
    "        elif(tag[1][:2] == 'JJ'):\n",
    "            word_list.append(lemma.lemmatize(tag[0], pos = wn.ADJ))\n",
    "    \n",
    "    return word_list\n",
    "\n",
    "def pre_processing (sentence):\n",
    "    return remove_stopwords(tokenize_sentences(remove_punctuation(sentence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in questa sezione sto dicendo che: data una parola in input, in questo caso Courage, la sua definizione ha lunghezza 1, e e in wordnet abbiamo solo 1 concetto associato\n",
    "def defintion_lenght(word):\n",
    "    lens = {}\n",
    "\n",
    "    for sys in wn.synsets(word):\n",
    "        sys_lenght=[]\n",
    "        l = len(sys.definition().split())\n",
    "        if lens.keys().__contains__(l):\n",
    "            lens[l] += 1\n",
    "        else:\n",
    "            lens[l]= 1\n",
    "    \n",
    "    ordered = dict(sorted(lens.items(), key=lambda item: item[0]))\n",
    "\n",
    "    print(ordered)\n",
    "    \n",
    "\n",
    "    #provo a plottare dei risultati \n",
    "    # using the variable ax for single a Axes\n",
    "    '''\n",
    "    fig, ax = matplotlib.pyplot.subplots()\n",
    "    ax.plot(ordered.keys(), ordered.values(), label='linear')\n",
    "    ax.set_xlabel('Lunghezza della definizione')\n",
    "    ax.set_ylabel('Count')\n",
    "    matplotlib.pyplot.show()\n",
    "    '''\n",
    "    #return lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------\n",
      "\n",
      "Concept:  Courage\n",
      "{15: 1}\n",
      "\n",
      "------------------------\n",
      "\n",
      "Concept:  Paper\n",
      "{3: 2, 5: 1, 6: 1, 8: 1, 11: 2, 14: 1, 15: 1}\n",
      "\n",
      "------------------------\n",
      "\n",
      "Concept:  Apprehension\n",
      "{2: 1, 4: 1, 7: 1, 8: 1}\n",
      "\n",
      "------------------------\n",
      "\n",
      "Concept:  Sharpener\n",
      "{14: 1}\n"
     ]
    }
   ],
   "source": [
    "for w in ['Courage', 'Paper', 'Apprehension', 'Sharpener']: \n",
    "    print(\"\\n------------------------\\n\")\n",
    "    print(\"Concept: \", w)\n",
    "    defintion_lenght(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_antonym_mero(word):\n",
    "    hyper_count = 0\n",
    "    mero_count = 0\n",
    "    antonym_count = 0\n",
    "\n",
    "    for sys in wn.synsets(word):\n",
    "        definition = sys.definition().split()\n",
    "        hypernym = set(sys.hypernyms())\n",
    "        antonyms = []\n",
    "\n",
    "        for l in sys.lemmas():\n",
    "            antonyms += l.antonyms()\n",
    "        antonyms = list(map(lambda x : x.synset(), antonyms))\n",
    "        meronyms =sys.part_meronyms()\n",
    "        meronyms += sys.substance_meronyms()\n",
    "\n",
    "        senses = []\n",
    "\n",
    "        for w in definition:\n",
    "            senses += wn.synsets(w)\n",
    "        if (set(senses).intersection(hypernym)):\n",
    "            hyper_count+=1\n",
    "        if(set(senses).intersection(antonyms)):\n",
    "            antonym_count+=1\n",
    "        if(set(senses).intersection(meronyms)):\n",
    "            mero_count+=1       \n",
    "    \n",
    "    print(\"\\ntot definizioni :\" + str(len(wn.synsets(word))) + \"\\tiperonimi trovati : \" + str(hyper_count) + \"\\t==> \" +str(hyper_count/len(wn.synsets(word))))\n",
    "    print(\"tot definizioni : \" + str(len(wn.synsets(word))) + \"\\tmeronimi trovati : \" + str(mero_count) + \"\\t==> \" +str(mero_count/len(wn.synsets(word))))\n",
    "    print(\"tot definizioni : \" + str(len(wn.synsets(word))) + \"\\tantonimi trovati : \" + str(antonym_count) + \"\\t==> = \" +str(antonym_count/len(wn.synsets(word))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------\n",
      "\n",
      "Concept:  Courage\n",
      "\n",
      "tot definizioni :1\tiperonimi trovati : 1\t==> 1.0\n",
      "tot definizioni : 1\tmeronimi trovati : 0\t==> 0.0\n",
      "tot definizioni : 1\tantonimi trovati : 0\t==> = 0.0\n",
      "\n",
      "------------------------\n",
      "\n",
      "Concept:  Paper\n",
      "\n",
      "tot definizioni :9\tiperonimi trovati : 7\t==> 0.7777777777777778\n",
      "tot definizioni : 9\tmeronimi trovati : 1\t==> 0.1111111111111111\n",
      "tot definizioni : 9\tantonimi trovati : 0\t==> = 0.0\n",
      "\n",
      "------------------------\n",
      "\n",
      "Concept:  Apprehension\n",
      "\n",
      "tot definizioni :4\tiperonimi trovati : 1\t==> 0.25\n",
      "tot definizioni : 4\tmeronimi trovati : 0\t==> 0.0\n",
      "tot definizioni : 4\tantonimi trovati : 0\t==> = 0.0\n",
      "\n",
      "------------------------\n",
      "\n",
      "Concept:  Sharpener\n",
      "\n",
      "tot definizioni :1\tiperonimi trovati : 1\t==> 1.0\n",
      "tot definizioni : 1\tmeronimi trovati : 0\t==> 0.0\n",
      "tot definizioni : 1\tantonimi trovati : 0\t==> = 0.0\n"
     ]
    }
   ],
   "source": [
    "for w in ['Courage', 'Paper', 'Apprehension', 'Sharpener']: \n",
    "    print(\"\\n------------------------\\n\")\n",
    "    print(\"Concept: \", w)\n",
    "    hyper_antonym_mero(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_hypernym_path(word):\n",
    "    lens = []\n",
    "\n",
    "    for sys in wn.synsets(word):\n",
    "        path=[]\n",
    "\n",
    "        hyp_path = sys.hypernym_paths()\n",
    "\n",
    "        for i in range(0, len(hyp_path[0])):\n",
    "            path.append((hyp_path[0][i], len((hyp_path[0][i].definition()).split())))\n",
    "            #path.append(hyp_path[0][i])\n",
    "\n",
    "        print(path)\n",
    "        lens.append(path)\n",
    "\n",
    "    return lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------\n",
      "\n",
      "Concept:  Courage\n",
      "[(Synset('entity.n.01'), 17), (Synset('abstraction.n.06'), 11), (Synset('attribute.n.02'), 9), (Synset('trait.n.01'), 7), (Synset('character.n.03'), 18), (Synset('spirit.n.03'), 9), (Synset('courage.n.01'), 15)]\n",
      "\n",
      "------------------------\n",
      "\n",
      "Concept:  Paper\n",
      "[(Synset('entity.n.01'), 17), (Synset('physical_entity.n.01'), 6), (Synset('matter.n.03'), 7), (Synset('substance.n.01'), 11), (Synset('material.n.01'), 12), (Synset('paper.n.01'), 15)]\n",
      "[(Synset('entity.n.01'), 17), (Synset('abstraction.n.06'), 11), (Synset('communication.n.02'), 12), (Synset('written_communication.n.01'), 10), (Synset('writing.n.02'), 24), (Synset('essay.n.01'), 6), (Synset('composition.n.08'), 8)]\n",
      "[(Synset('entity.n.01'), 17), (Synset('physical_entity.n.01'), 6), (Synset('object.n.01'), 12), (Synset('whole.n.02'), 11), (Synset('artifact.n.01'), 7), (Synset('instrumentality.n.03'), 13), (Synset('medium.n.01'), 9), (Synset('print_media.n.01'), 6), (Synset('press.n.02'), 16), (Synset('newspaper.n.01'), 14)]\n",
      "[(Synset('entity.n.01'), 17), (Synset('physical_entity.n.01'), 6), (Synset('object.n.01'), 12), (Synset('whole.n.02'), 11), (Synset('artifact.n.01'), 7), (Synset('instrumentality.n.03'), 13), (Synset('medium.n.01'), 9), (Synset('paper.n.04'), 5)]\n",
      "[(Synset('entity.n.01'), 17), (Synset('abstraction.n.06'), 11), (Synset('communication.n.02'), 12), (Synset('expressive_style.n.01'), 25), (Synset('writing_style.n.01'), 7), (Synset('prose.n.01'), 6), (Synset('nonfiction.n.01'), 6), (Synset('article.n.01'), 9), (Synset('paper.n.05'), 11)]\n",
      "[(Synset('entity.n.01'), 17), (Synset('abstraction.n.06'), 11), (Synset('group.n.01'), 9), (Synset('social_group.n.01'), 5), (Synset('organization.n.01'), 7), (Synset('enterprise.n.02'), 6), (Synset('business.n.01'), 11), (Synset('firm.n.01'), 14), (Synset('publisher.n.01'), 6), (Synset('newspaper.n.02'), 6)]\n",
      "[(Synset('entity.n.01'), 17), (Synset('physical_entity.n.01'), 6), (Synset('object.n.01'), 12), (Synset('whole.n.02'), 11), (Synset('artifact.n.01'), 7), (Synset('creation.n.02'), 10), (Synset('product.n.02'), 11), (Synset('newspaper.n.03'), 11)]\n",
      "[(Synset('cover.v.01'), 9), (Synset('paper.v.01'), 3)]\n",
      "[(Synset('cover.v.01'), 9), (Synset('wallpaper.v.01'), 3)]\n",
      "\n",
      "------------------------\n",
      "\n",
      "Concept:  Apprehension\n",
      "[(Synset('entity.n.01'), 17), (Synset('abstraction.n.06'), 11), (Synset('attribute.n.02'), 9), (Synset('state.n.02'), 10), (Synset('feeling.n.01'), 7), (Synset('emotion.n.01'), 3), (Synset('fear.n.01'), 20), (Synset('apprehension.n.01'), 4)]\n",
      "[(Synset('entity.n.01'), 17), (Synset('abstraction.n.06'), 11), (Synset('psychological_feature.n.01'), 10), (Synset('cognition.n.01'), 9), (Synset('process.n.02'), 14), (Synset('higher_cognitive_process.n.01'), 13), (Synset('knowing.n.01'), 6), (Synset('understanding.n.01'), 7)]\n",
      "[(Synset('entity.n.01'), 17), (Synset('abstraction.n.06'), 11), (Synset('psychological_feature.n.01'), 10), (Synset('cognition.n.01'), 9), (Synset('content.n.05'), 12), (Synset('belief.n.01'), 6), (Synset('expectation.n.01'), 8), (Synset('apprehension.n.03'), 2)]\n",
      "[(Synset('entity.n.01'), 17), (Synset('abstraction.n.06'), 11), (Synset('psychological_feature.n.01'), 10), (Synset('event.n.01'), 9), (Synset('act.n.02'), 8), (Synset('acquiring.n.01'), 5), (Synset('capture.n.01'), 9), (Synset('apprehension.n.04'), 8)]\n",
      "\n",
      "------------------------\n",
      "\n",
      "Concept:  Sharpener\n",
      "[(Synset('entity.n.01'), 17), (Synset('physical_entity.n.01'), 6), (Synset('object.n.01'), 12), (Synset('whole.n.02'), 11), (Synset('artifact.n.01'), 7), (Synset('instrumentality.n.03'), 13), (Synset('implement.n.01'), 12), (Synset('sharpener.n.01'), 14)]\n"
     ]
    }
   ],
   "source": [
    "for w in ['Courage', 'Paper', 'Apprehension', 'Sharpener']: \n",
    "    print(\"\\n------------------------\\n\")\n",
    "    print(\"Concept: \", w)\n",
    "    total_hypernym_path(w)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8713ca3b77628a4414cb19afecce4be886be5a2f883480e0c3ed66d57c8d45ba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
