{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_rows_csv(csv_file):\n",
    "    for rows in csv_file:\n",
    "        return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(sentence):\n",
    "    return re.sub(r'[^\\w\\s]', '', sentence)\n",
    "\n",
    "def remove_stopwords(words_list):\n",
    "    stopwords = open(\"stop_words_FULL.txt\", \"r\")\n",
    "    stopwords_list = []\n",
    "    for word in stopwords:\n",
    "        stopwords_list.append(word.replace('\\n', ''))\n",
    "    stopwords.close()\n",
    "    return [value.lower() for value in words_list if value.lower() not in stopwords_list]\n",
    "    \n",
    "#uso il tokenize_sentence perche poi mi servira' in futuro per la gestione delle frasi, con la relativa ricerca della parola piu frequente in una frase\n",
    "def tokenize_sentences(sentence):\n",
    "    word_list= []\n",
    "    lemma = WordNetLemmatizer()\n",
    "    for tag in nltk.pos_tag(word_tokenize(sentence)):\n",
    "        if(tag[1][:2] == 'NN'):\n",
    "            word_list.append(lemma.lemmatize(tag[0], pos = wn.NOUN))\n",
    "        elif(tag[1][:2] == 'VB'):\n",
    "            word_list.append(lemma.lemmatize(tag[0], pos = wn.VERB))\n",
    "        elif(tag[1][:2] == 'RB'):\n",
    "            word_list.append(lemma.lemmatize(tag[0], pos = wn.ADV))\n",
    "        elif(tag[1][:2] == 'JJ'):\n",
    "            word_list.append(lemma.lemmatize(tag[0], pos = wn.ADJ))\n",
    "    \n",
    "    return word_list\n",
    "\n",
    "def pre_processing (sentence):\n",
    "    return remove_stopwords(tokenize_sentences(remove_punctuation(sentence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def definitions(file):\n",
    "\n",
    "    readCSV = csv.reader(file, delimiter=',')\n",
    "    \n",
    "    # get list of words to analize\n",
    "    words = read_rows_csv(readCSV)[1:]\n",
    "\n",
    "    definitions_words = dict()\n",
    "\n",
    "    for row in readCSV:\n",
    "        for index, definition in enumerate(row):\n",
    "            # controllo se e' presente o no una definizione\n",
    "            if definition:  \n",
    "                if index > 0:\n",
    "                    word = words[index - 1]\n",
    "                    if word not in definitions_words.keys():\n",
    "                        definitions_words[word] = [pre_processing(definition)]\n",
    "                    else:\n",
    "                        definitions_words[word].append(pre_processing(definition))\n",
    "    return definitions_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#la formula per calcolare la distanza del coseno e': Cos(x, y) = x . y / ||x|| * ||y||\n",
    "def cosine_similarity(definition1, defintion2):\n",
    "    vector1=[]\n",
    "    vector2=[]\n",
    "    \n",
    "    total_def = (set(definition1) | set(defintion2)) #creo un insieme con sia definition 1 che definition 2\n",
    "\n",
    "#creo il vettore\n",
    "    for w in total_def:\n",
    "        if w in definition1:\n",
    "            vector1.append(1)\n",
    "        else:\n",
    "            vector1.append(0)\n",
    "        \n",
    "        if w in defintion2:\n",
    "            vector2.append(1)\n",
    "        else:\n",
    "            vector2.append(0)\n",
    "    \n",
    "    c = 0\n",
    "\n",
    "    #cosine formula \n",
    "    for i in range(len(total_def)):\n",
    "        c += vector1[i] * vector2[i]\n",
    "    \n",
    "    cosine = c / float((sum(vector1) * sum(vector2)) ** 0.5)\n",
    "\n",
    "    #print(\"similarity :\", cosine)\n",
    "\n",
    "    return cosine    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_results(total_def):\n",
    "    results = dict() #dizionario con tutte le liste di definizioni processate\n",
    "    for word in total_def.keys(): \n",
    "        definitions = total_def[word] \n",
    "        \n",
    "        avg = 0\n",
    "        count = 0\n",
    "        for def1 in definitions:\n",
    "            for def2 in definitions:\n",
    "                if not def1 == def2:\n",
    "                    avg += cosine_similarity(def1, def2)\n",
    "                    count += 1\n",
    "        \n",
    "        results[word] = avg / count\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_frequent_words(definitions):\n",
    "    words = set([word for definition in definitions for word in definition])\n",
    "\n",
    "    freq_words = dict()\n",
    "\n",
    "    for w in words:\n",
    "\n",
    "        for d in definitions:\n",
    "\n",
    "            if w in d:\n",
    "\n",
    "                if w not in freq_words.keys():\n",
    "                    freq_words[w]=1\n",
    "                else:\n",
    "                    count = freq_words[w] + 1\n",
    "                    freq_words[w]= count\n",
    "\n",
    "        if w in freq_words:\n",
    "            freq_words[w] += 1\n",
    "\n",
    "        else:\n",
    "            freq_words[w] = 1\n",
    "\n",
    "    \n",
    "    most_frequent_words = []\n",
    "    \n",
    "    for word in freq_words.keys():\n",
    "        if freq_words[word] >= (0.5 * len(definitions)):\n",
    "            most_frequent_words.append(word)\n",
    "\n",
    "    return most_frequent_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Similiarita del coseno :  {'Courage': 0.21054727554969985, 'Paper': 0.29258850377799267, 'Apprehension': 0.0830330313557733, 'Sharpener': 0.3863878711824424}\n",
      "\n",
      " Most Frequent words : \n",
      "[('Courage', ['ability', 'fear']), ('Paper', ['write', 'material']), ('Apprehension', []), ('Sharpener', ['tool', 'sharpen', 'pencil'])]\n"
     ]
    }
   ],
   "source": [
    "with open('defs.csv') as csvfile:\n",
    "    total_def = definitions(csvfile)\n",
    "    results = compute_results(total_def)\n",
    "    print (' Similiarita del coseno : ' , results)\n",
    "    print('\\n Most Frequent words : ')\n",
    "    print ([(key, most_frequent_words(total_def[key])) for key in ['Courage', 'Paper', 'Apprehension', 'Sharpener']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8713ca3b77628a4414cb19afecce4be886be5a2f883480e0c3ed66d57c8d45ba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
