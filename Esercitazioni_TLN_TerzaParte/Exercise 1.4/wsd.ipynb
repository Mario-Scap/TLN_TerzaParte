{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_set = set()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_words_path_set(path):\n",
    "    res = set()\n",
    "    with open(path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    for line in lines:\n",
    "        res.add(str(line.rstrip()))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_stopword_from_sentence(sentence):\n",
    "    relevant_words = set()\n",
    "    for s in sentence.split(' '):\n",
    "        if s not in stop_words_set:\n",
    "            relevant_words.add(remove_punctuation(s))\n",
    "    return relevant_words\n",
    "\n",
    "\n",
    "def remove_punctuation(string):\n",
    "    chars = '.,:;!?()”“…-'\n",
    "    for c in chars:\n",
    "        string = string.replace(c, '')\n",
    "    string = string.replace(\"’s\", '')\n",
    "    string = string.replace(\"’s\", '')\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_from_examples(examples):\n",
    "    words = set()\n",
    "    if len(examples) > 0:\n",
    "        for example in examples:\n",
    "            for word in example.split():\n",
    "                words.add(remove_punctuation(word))\n",
    "    return words\n",
    "\n",
    "\n",
    "def get_words_from_definition(param):\n",
    "    words = set()\n",
    "    for word in param.split(' '):\n",
    "        if words not in stop_words_set:\n",
    "            words.add(word)\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Lesk Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection(first_set, second_set):\n",
    "    intersection_set = set()\n",
    "    for var in first_set:\n",
    "        if var in second_set:\n",
    "            intersection_set.add(var)\n",
    "    return intersection_set\n",
    "\n",
    "\n",
    "def union(s1, s2):\n",
    "    res = set()\n",
    "    for var in s1:\n",
    "        res.add(var)\n",
    "    for var in s2:\n",
    "        if var not in res:\n",
    "            res.add(var)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_ctx(sense):\n",
    "    res = set()\n",
    "    words_examples = get_words_from_examples(sense.examples())\n",
    "    words_definition = get_words_from_definition(sense.definition())\n",
    "    contex = union(words_examples, words_definition)\n",
    "\n",
    "    # Hyponym contex\n",
    "    for hyponym in sense.hyponyms():\n",
    "        hyponym_ctx_word = union(get_words_from_examples(hyponym.examples()),\n",
    "                                 get_words_from_definition(hyponym.definition()))\n",
    "        contex = union(hyponym_ctx_word, contex)\n",
    "\n",
    "    # hypernym contex\n",
    "    for hypernym in sense.hypernyms():\n",
    "        hypernym_ctx_word = union(get_words_from_examples(hypernym.examples()),\n",
    "                                  get_words_from_definition(hypernym.definition()))\n",
    "        contex = union(hypernym_ctx_word, contex)\n",
    "\n",
    "    # Filter stopword\n",
    "    for w in contex:\n",
    "        if w not in stop_words_set:\n",
    "            res.add(w)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lesk_algorithm(word, sentence):\n",
    "    senses = wn.synsets(word.strip())\n",
    "    if len(senses) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        best_sense = wn.synsets(word)[0]\n",
    "        max_overlap = set()\n",
    "        context = filter_stopword_from_sentence(sentence)\n",
    "        for sense in wn.synsets(word):\n",
    "            signature = get_wordnet_ctx(sense)\n",
    "            overlap = intersection(signature, context)\n",
    "            if len(overlap) > len(max_overlap):\n",
    "                max_overlap = overlap\n",
    "                best_sense = sense\n",
    "        return best_sense"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8713ca3b77628a4414cb19afecce4be886be5a2f883480e0c3ed66d57c8d45ba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
